[{"content":"Why do you remember certificates As an IT professional, you might have come across this error while browsing sites of setting up your local development environment. As a fix, you would have been recommended to go to the advanced option in this error and accept the risk and allow the website to work. Your browser may still warn you by displaying the lock on the ominbar of your website as red or unsafe, but you choose to ignore it and not deal with it.\nWhy do you see this error? Browsers don\u0026rsquo;t do a very good job of explaining the error upfront. Only if you go on the Learn more link, you might hope to get some more information about the error. Sometimes it may confuse you more.\nWhat does the above error mean The error message says NET::ERR_CERT_INVALID, but what does that mean. It\u0026rsquo;s doesn\u0026rsquo;t tell what\u0026rsquo;s invalid and why. If you venture more and click on the error, you will be presented with this information.\nSubject: *.badssl.com Issuer: *.badssl.com Expires on: 9 Oct 2021 Current date: 8 Aug 2021 PEM encoded chain: -----BEGIN CERTIFICATE----- MIIDeTCCAmGgAwIBAgIJAPziuikCTox4MA0GCSqGSIb3DQEBCwUAMGIxCzAJBgNV ---SNIPPED--- EVA0pmzIzgBg+JIe3PdRy27T0asgQW/F4TY61Yk= -----END CERTIFICATE-----  This shows a bunch of information which may not make sense. What is a subject, CERTIFICATE, the gibberish between the markers?\nHow is information protected on the internet Internet in the early days was open for anyone to view and by that I mean, anyone could read the communication between 2 parties. This is not ok when you are transferring sensitive information, something like bank transactions. To safeguard the traffic on the wire, we started to encrypt the traffic which could only be decrypted by the 2 communicating parties. In order to encrypt the information, we need some secret that is known only to the involved parties. Actually there are some requirements that you should look for\n Two parties could talk securely over and insecure channel Both the parties could identify and verify each other  Enters Asymmetric encryption Asymmetric encryption is a scheme involving a key which consists of 2 parts, a public and a private part. The private part should be protected by the owner and the public part can be distributed to everyone. A public key can be used to encrypt information which can only be decrypted by the corresponding private key.\nNow everyone can have their own key pairs and use it to share information on an insecure channel. If Alice wants to share information with Bob, Alice can encrypt is using Bob\u0026rsquo;s public key and send it over the wire and only Bob could use his private key to decrypt it and make sense of it. This solves the first requirement.\nThis constitues Public key cryptography which provided 2 features\n Public key encryption - Encryption happens using a public key (available to all) and decryption happens using a private key (owned by a single entity). Digital signature - A methematically proven method to prove ownership of the private key without exposing anything.  There is still a problem, they haven\u0026rsquo;t shared or proven their identity, which means, Alice is sharing information securely with someone. There is no way to prove that it\u0026rsquo;s Bob unless they are aware of each other\u0026rsquo;s public keys before hand which is not practical when there are so many people to talk to.\nThis problem is managed by the Public Key Infrastructure. It allows to put identity to public keys.\nPublic Key Infrastructure Public key infrastructure is a system of vetting a public keys by a trusted third party. For example, between Alice, Bob and Charlie, if both Alice and Bob trust Charlie, they are going to trust Charlie\u0026rsquo;s word on which public keys belong to whom. Charlie here will be termed as a Certificate Authority or an Issuer in this context.\nTechnically it\u0026rsquo;s a chicken an egg problem, how do you trust Charlie without meeting them. But that\u0026rsquo;s a topic for another day.\nBack to the original problem Notice the error message on the image, It says,\nSubject: *.badssl.com Issuer: *.badssl.com  Chrome does not trust *.badssl.com as an Issuer, so it\u0026rsquo;s not going to trust anything vetted by it and hence is rejecting the connection.\nWhat\u0026rsquo;s that strange blob of text below it PKI provides a way to attach an identity to a public key. But how does it do that? Remember how the is a common trusted third party. The third party digitally signs the identity of the private key holder and their public key and distributes it to everyone who wants it.\nThe CERTIFICATE is a public document to identify the owner of the private part of the public key. You may choose to trust or not trust the third party who is vouching for this certification.\nInto the weeds, how do we actually make sense of the strange blob. Here\u0026rsquo;s how\necho \u0026quot;-----BEGIN CERTIFICATE----- MIIDeTCCAmGgAwIBAgIJAPziuikCTox4MA0GCSqGSIb3DQEBCwUAMGIxCzAJBgNV BAYTAlVTMRMwEQYDVQQIDApDYWxpZm9ybmlhMRYwFAYDVQQHDA1TYW4gRnJhbmNp c2NvMQ8wDQYDVQQKDAZCYWRTU0wxFTATBgNVBAMMDCouYmFkc3NsLmNvbTAeFw0x OTEwMDkyMzQxNTJaFw0yMTEwMDgyMzQxNTJaMGIxCzAJBgNVBAYTAlVTMRMwEQYD VQQIDApDYWxpZm9ybmlhMRYwFAYDVQQHDA1TYW4gRnJhbmNpc2NvMQ8wDQYDVQQK DAZCYWRTU0wxFTATBgNVBAMMDCouYmFkc3NsLmNvbTCCASIwDQYJKoZIhvcNAQEB BQADggEPADCCAQoCggEBAMIE7PiM7gTCs9hQ1XBYzJMY61yoaEmwIrX5lZ6xKyx2 PmzAS2BMTOqytMAPgLaw+XLJhgL5XEFdEyt/ccRLvOmULlA3pmccYYz2QULFRtMW hyefdOsKnRFSJiFzbIRMeVXk0WvoBj1IFVKtsyjbqv9u/2CVSndrOfEk0TG23U3A xPxTuW1CrbV8/q71FdIzSOciccfCFHpsKOo3St/qbLVytH5aohbcabFXRNsKEqve ww9HdFxBIuGa+RuT5q0iBikusbpJHAwnnqP7i/dAcgCskgjZjFeEU4EFy+b+a1SY QCeFxxC7c3DvaRhBB0VVfPlkPz0sw6l865MaTIbRyoUCAwEAAaMyMDAwCQYDVR0T BAIwADAjBgNVHREEHDAaggwqLmJhZHNzbC5jb22CCmJhZHNzbC5jb20wDQYJKoZI hvcNAQELBQADggEBAGlwCdbPxflZfYOaukZGCaxYK6gpincX4Lla4Ui2WdeQxE95 w7fChXvP3YkE3UYUE7mupZ0eg4ZILr/A0e7JQDsgIu/SRTUE0domCKgPZ8v99k3A vka4LpLK51jHJJK7EFgo3ca2nldd97GM0MU41xHFk8qaK1tWJkfrrfcGwDJ4GQPI iLlm6i0yHq1Qg1RypAXJy5dTlRXlCLd8ufWhhiwW0W75Va5AEnJuqpQrKwl3KQVe wGj67WWRgLfSr+4QG1mNvCZb2CkjZWmxkGPuoP40/y7Yu5OFqxP5tAjj4YixCYTW EVA0pmzIzgBg+JIe3PdRy27T0asgQW/F4TY61Yk= -----END CERTIFICATE-----\u0026quot; | openssl x509 -noout -text  The above command dumps the following information\nCertificate: Data: Version: 3 (0x2) Serial Number: 18222331727589575800 (0xfce2ba29024e8c78) Signature Algorithm: sha256WithRSAEncryption Issuer: C=US, ST=California, L=San Francisco, O=BadSSL, CN=*.badssl.com Validity Not Before: Oct 9 23:41:52 2019 GMT Not After : Oct 8 23:41:52 2021 GMT Subject: C=US, ST=California, L=San Francisco, O=BadSSL, CN=*.badssl.com Subject Public Key Info: Public Key Algorithm: rsaEncryption Public-Key: (2048 bit) Modulus: 00:c2:04:ec:f8:8c:ee:04:c2:b3:d8:50:d5:70:58: ---SNIPPED--- 7c:f9:64:3f:3d:2c:c3:a9:7c:eb:93:1a:4c:86:d1: ca:85 Exponent: 65537 (0x10001) X509v3 extensions: X509v3 Basic Constraints: CA:FALSE X509v3 Subject Alternative Name: DNS:*.badssl.com, DNS:badssl.com Signature Algorithm: sha256WithRSAEncryption 69:70:09:d6:cf:c5:f9:59:7d:83:9a:ba:46:46:09:ac:58:2b: ---SNIPPED--- 00:60:f8:92:1e:dc:f7:51:cb:6e:d3:d1:ab:20:41:6f:c5:e1: 36:3a:d5:89  Notice the following things\n Serial Number: The unique identifier for the certificate with the issuer Issuer: The identity of who vetted the public key Subject: The identity of how owns the private key Subject Public key: The actual public key which is vetted Signature Algorithm: The algorithm used to sign the identity and public key information Validity: The dates for which this identity is considered valid  This is an RSA keypair. So the public key has a Modulus and an Exponent. These values can be use to encrypt a message.\nThe signature Algorithm is the algorithm used to digitally sign the information in the certificate so that anyone can verify the validity of the certificate. This verification is done by the public key of the Issuer or the trusted third party who we already trust.\nHow does a trusted certificate look like The certificate above is a certificate that I don\u0026rsquo;t trust, but how does a certificate look like that I trust. Let\u0026rsquo;s see what www.google.com presents when we try to connect to it.\necho | openssl s_client -connect www.google.com:443 2\u0026gt;/dev/null \\ | openssl x509 -noout -text  This looks something like\nCertificate: Data: Version: 3 (0x2) Serial Number: 3a:6a:c0:d8:53:c9:8f:f1:0a:00:00:00:00:f2:c8:bb Signature Algorithm: sha256WithRSAEncryption Issuer: C=US, O=Google Trust Services LLC, CN=GTS CA 1C3 Validity Not Before: Jul 12 03:48:05 2021 GMT Not After : Oct 4 03:48:04 2021 GMT Subject: CN=www.google.com Subject Public Key Info: Public Key Algorithm: rsaEncryption Public-Key: (2048 bit) Modulus: 00:b5:87:a3:12:1a:60:3d:6f:74:f1:33:0f:67:60: ---SNIPPED-- 11:3e:1f:ba:8b:59:d9:e8:6c:a0:6c:3e:07:31:e9: 56:bb Exponent: 65537 (0x10001) X509v3 extensions: X509v3 Key Usage: critical Digital Signature, Key Encipherment X509v3 Extended Key Usage: TLS Web Server Authentication X509v3 Basic Constraints: critical CA:FALSE X509v3 Subject Key Identifier: 2D:42:0A:99:66:0A:65:08:C7:B4:59:B8:23:55:13:FA:A2:D1:A2:01 X509v3 Authority Key Identifier: keyid:8A:74:7F:AF:85:CD:EE:95:CD:3D:9C:D0:E2:46:14:F3:71:35:1D:27 Authority Information Access: OCSP - URI:http://ocsp.pki.goog/gts1c3 CA Issuers - URI:http://pki.goog/repo/certs/gts1c3.der X509v3 Subject Alternative Name: DNS:www.google.com X509v3 Certificate Policies: Policy: 2.23.140.1.2.1 Policy: 1.3.6.1.4.1.11129.2.5.3 X509v3 CRL Distribution Points: Full Name: URI:http://crls.pki.goog/gts1c3/fVJxbV-Ktmk.crl 1.3.6.1.4.1.11129.2.4.2: ......u.\\.C....ED.^..V..7...G..s..^........z..4=.....F0D.. ..4.8...xK.].rk.....o.R ..^.T%.S..vt11..v..\\./.w0\u0026quot;.T..0.V..M..3.../ ..N.d....z..3......G0E. ...|..Y...R2.......k...+.....8.p.!..SF...+.o..').D.........mc....(A Signature Algorithm: sha256WithRSAEncryption d7:9f:97:e8:2a:5c:bc:04:9d:51:9d:ab:d4:9b:b0:cd:71:b5: ---SNIPPED--- 45:b0:52:39  This certificate has a lot more information about the the Subject and what this key is allowed to do.\nA scary view of the certificate Try the following commands\necho | openssl s_client -connect www.google.com:443 2\u0026gt;/dev/null \\ | openssl x509 \\ | openssl asn1parse -i echo | openssl s_client -connect www.google.com:443 2\u0026gt;/dev/null \\ | openssl x509 \\ | openssl asn1parse -i -strparse 196  Look for interesting stuff in the output!\nNow that we know how the certificate look and what does it contain, let\u0026rsquo;s see how to use it for a local development environment.\nUsing certificates in a local development environment Using self signed certificates can be a pain for 2 reasons\n There can be a lot depding on what domains you are issuing them for You will have to deal with pesky errors from browsers and clients setting to ignore SSL verification everywhere which isn\u0026rsquo;t ideal.  Since you control your development environment and is confined to your development environment, why not just become the Certificate Authority and use it to issue all your dev certificates. You trust just one certificate once and any certificate issued by that third party (which is you) is automatically trusted by your clients.\nOk, easier said than. How do I issue my own Certificate Authority.\n As they say, Hold my beer!\n Note: If you want to follow along, then you will require Golang version 1.12 or above.\n# Assuming you have Golang installed git clone https://github.com/square/certstrap cd certstrap go build # create a directory for your binaries, useful for other stuff as well mkdir \u0026quot;$HOME/.bin\u0026quot; mv certstrap \u0026quot;$HOME/.bin/certstrap\u0026quot; # put certstrap in the PATH export PATH=\u0026quot;$PATH:$HOME/.bin\u0026quot; certstrap --version  If your package manager has certstrap, it\u0026rsquo;s even simpler. And that\u0026rsquo;s it, you are good to issue certificates\nGenerate your Certificate authority certstrap init --common-name domain.com \\ --organization Domain \\ --organizational-unit AllThingsSecure \\ --country IN \\ --province KA  Enter the passphrase prompted. You always want to protect your CAs because it will essentially become a trusted certificate and if this leaks, bad things can happen to you, like impersonating Gmail or your banks! Be careful!!\nThe above command should dump out 3 files\nCreated out/domain.com.key Created out/domain.com.crt Created out/domain.com.crl  These are your Key, Certificate and Certificate Revocation List (used to revoke certificates issued by this CA if you choose to, basically to say that I no longer vet a particular certificate).\nIssue your first certificate Once your CA is issued, you can use it issue a certificate.\ncertstrap request-cert \\ --common-name foo.domain.com \\ --domain foo.domain.com,bar.domain.com \\ --country IN --organization Domain --organizational-unit NewTeam  Note: Make sure to have the Common Name in the --domain flag since Chrome will reject if the CN is not part of the SAN attributes as browsers no longer trust the CN field alone.\nThe above command is going to generate 2 files.\nCreated out/foo.domain.com.key Created out/foo.domain.com.csr  One of them is an RSA private key and the other is a CSR or a Certificate Signing Request. This is like a form you fill up and ask the Certificate Authority to certify. A real CA may do it\u0026rsquo;s own verification to make sure the information is correct. In this case, since we are the CA, we know it\u0026rsquo;s trustworthy. So, we certify by signing it using the CA key like below\ncertstrap sign foo.domain.com \\ --CA domain.com \\ --expires \u0026quot;3 months\u0026quot; --csr out/foo.domain.com.csr  Answer all the password prompts, and there you have it, your certificate. You ideally want to issue short lived certificates for dev usage but you can issue for a longer duration but not longer than 397 days. You generate the CA once and then use the request-cert and sign step to generate as many certs you want.\nWhat if I need more info in the certficiate This is a very simple way to generate certificates. This is likely sufficient for a local web development environment but if you have complicated needs which require different attributes, encoding addition and custom information, you are likely going to need a better way. openssl certificate authority is an excellent resource to build you certificates with all the knobs exposed.\nMake your system trust your CA All operating systems have a default list of certificates that it trusts, they are usually very well known and trusted CAs. The browser and clients can either refer to the Operating system list for trusting CAs or have their own list. For example,\n Chrome uses the system default list Firefox has its own list NodeJS has it\u0026rsquo;s own list and for any new CAs it uses the file pointed by NODE_EXTRA_CA_CERTS.  You can refer to your platform documentation, on how to import/add a new trusted certificate. For MacOS, it\u0026rsquo;s pretty straight forward, just add it in the Keychain Access application in the System list. For Linux, refer to the man page for update-ca-certificates utility.\nWindows users are on their own!\nOnce you have the CA added, all of your applications should trust all certificates issued by your CA.\nThe issuer heirarchy As mentioned, in general CA keys are kept in a highly secure location with no access to the network. They are usually valid for more than 10 years which means loosing them would cause a huge chaos. How do we then use it to issue certificates. To solve this problem, other certificates are issued called Issuers which are valid for a smaller time duration and are allowed to issue other Issuers or certificates called leaf certificates as they are no longer allowed to issue other certs, so last in the tree of certs or the leaf.\necho | openssl s_client -connect google.com:443 \u0026gt;/dev/null  produces,\ndepth=3 C = BE, O = GlobalSign nv-sa, OU = Root CA, CN = GlobalSign Root CA verify return:1 depth=2 C = US, O = Google Trust Services LLC, CN = GTS Root R1 verify return:1 depth=1 C = US, O = Google Trust Services LLC, CN = GTS CA 1C3 verify return:1 depth=0 CN = *.google.com verify return:1 DONE  Notice how the server presents 4 certificates,\n C = BE, O = GlobalSign nv-sa, OU = Root CA, CN = GlobalSign Root CA The Root CA C = US, O = Google Trust Services LLC, CN = GTS Root R1 an intermediate issuer CA C = US, O = Google Trust Services LLC, CN = GTS CA 1C3 another layer of intermediate issuer CA CN = *.google.com the final leaf certificate for the server  What does a certificate contain Now that you have seen how to make a simple certificate chain, let\u0026rsquo;s see what each item in a sample certificate mean. Looking at the www.google.com certificate,\n Version denotes the certificate structure, version 3 is the x509v3 type of certificate. Serial Number notes the unique identifier for the certificate. This can be same for 2 different issuers but can\u0026rsquo;t be same for any 2 certificate for a single issuer. Signature Algorithm defines what algorithm is used to digitally sign the data in the certificate by the Issuer. sha256WithRSAEncryption means that the data was hashed using SHA256 and then encrypted using the private key of the issuer (the signature scheme uses private key for \u0026ldquo;encryption\u0026rdquo; instead of the public key like in the standard encryption scheme). Issuer defines the fully qualified name of the Issuer of the certificate. Usually a CA does not use it\u0026rsquo;s root private key for any operation, it will issue a group of Issuer certificates which are allowed to sign other certificates for practical considerations. The CA private keys are kept under the highest security off the network. Validity defines the time period for which this certificate is valid, beyond this, the certificate should not be trusted. Subject defines the fully qualified descriptor of the identity who is claiming to be the owner of the private part of the public key in the certificate. Subject Public Key Info defines what algorithm and respective values are representing the key. The algorithm is rsaEncryption which contains of a modulus and an exponent. Refer the Wiki article about RSA key pair. X509v3 extensions represent what a particular certificate is allowed to do. For example   X509v3 Key Usage, a critical extension which should be strictly adhered to. It\u0026rsquo;s allowed to do Digital Signature (allowed to sign some data digitally), Key Encipherment (encrypt other keys not part of the application, like in TLS)\n  X509v3 Extended Key Usage allows this certificate to be used as a TLS Web Server Authentication, which is this certificate can be used to represent a client\u0026rsquo;s identity during a TLS connection\n  X509v3 Basic Constraints, a critical extension, says, this certificate should not be considered as a CA or a certificate authority.\n  X509v3 Subject Key Identifier defines identity of the private key. Certificates expire, which means, a new key pair can represent the same identity, so the private keys are differentiated by this value.\n  Authority Key Identifier defines the identity of the private key of the signer which vetted this certificate.\n  Authority Information Access tells locations on the internet where to find more information about the issuer.\n OCSP or Online Certificate Status Protocol is a protocol that allows a client to check if the certificate which is presented is actually valid and not revoked CA Issuers has the information about the issuer certificate if it\u0026rsquo;s not bundled with this certificate. This can be handy to verify the certificate to the root CA curl http://pki.goog/repo/certs/gts1c3.der -o - \\ | openssl x509 -inform DER -noout -text      X509v3 Subject Alternative Name, in case this certificate is used to serve a TLS request against a domain or DNS name, what DNS names can be represented by this certificate. So, if the DNS names contain foo.google.com and bar.google.com, the certificate will be allowed to represent those domains. But, if this certificate is presented during a request with facebook.com, the client will reject it because it\u0026rsquo;s not part of the DNS names.\n  X509v3 Certificate Policies is a set of addition (possibly custom) policies which may be enforced on this certificate. Note that it doesn\u0026rsquo;t have an \u0026ldquo;readable\u0026rdquo; name. This could signify that it\u0026rsquo;s a non-standard attribute not known to the generic public client and may be ignored.\n 2.23.140.1.2.1 denotes  Use of this OID is appropriate when the certificate lacks information about the certificate holder\u0026rsquo;s legal identity.\n  1.3.6.1.4.1.11129.2.5.3 seems to be a custom value, still part of google(11129) though.    X509v3 CRL distribution Points, a CA may want to say that these certificate (issuers or leaf certificates) are no longer valid because of some issue other than expiry. That information is encoded in a Certificate Revocation List which is published by each CA and should be checked by a client if possible to make sure it\u0026rsquo;s trusting a valid certificate.\ncurl http://crls.pki.goog/gts1c3/fVJxbV-Ktmk.crl -o - \\ | openssl crl -inform DER -noout -text  Notice how this literally has a list of Serial Numbers of the certs that will be no longer valid after the Revocation Date.\n  1.3.6.1.4.1.11129.2.4.2, Eh? what\u0026rsquo;s this. This is a custom (non-standard) attribute represnted in an OID format. This OID refers to\n{ iso(1) identified-organization(3) dod(6) internet(1) private(4) enterprise(1) google(11129) 2(2) 4(4) 2(2) }     Signature Algorithm this time defines what signature algorithm is used and what is it\u0026rsquo;s value.  This by no means is an exhaustive list of what can a certificate contain. In theory, a certificate is just a container and can contain any information you like. For a simple local development use case, this article should take you far enough to not worry about it later.\nFor any usecase that requires exposing your applications over the public internet or with a large group of people, I would highly recommend to use a more \u0026ldquo;professional\u0026rdquo; and battle tested CA. Some of them are\n Let\u0026rsquo;s Encrypt the most popular production-ready and free CA Cloudflare cfssl an opensource CA which could be ideal for a staging style scenario Then there are always native CAs for the cloud environment like the Amazon CA, Google Trust Services, Global Sign and many more.  ","date":"2021-08-12","permalink":"https://prateeknischal.github.io/posts/trust-the-certs/","tags":null,"title":"Trust the Certs"},{"content":"There is a certain confidence boost when you see that message \u0026ldquo;All tests pass\u0026rdquo;, unless you are like me and get lazy, skimping on writing good tests for your logic. There are a lot of tools integrated with github, gitlab and probably other git interfaces that run a set of pre-defined tasks before potentially allowing you to raise a PR or mark it safe for merge. For the uninitiated, it\u0026rsquo;s know as CI or continuous integration tools. For example,\n Azure pipelines Github actions CicleCI TravisCI  What if you could run those tests, lint checks and other stuff before you push your commit and realize it\u0026rsquo;s going to fail in your CI pipeline and make you look like a n00b. Wouldn\u0026rsquo;t that be great !\nUnfortunately, there aren\u0026rsquo;t a lot of tools that do the same for plain old git, I mean locally in your development environment. There is one for nodejs, called the pre-commit, but let\u0026rsquo;s ignore that since we are gophers here.\nThere is a project pre-commit/pre-commit that can be used to manage a wide variety of pre-commit stuff and in a pretty way. It looks really good ! The only gripe that I have with it (highly personal) is it needs a runtime. I am not saying that runtimes are bad, they just feel a bit bloated and then you run into their dependencies and versions and give up. The most painful transition for me has been python2.x to python3.x. So, I try to avoid python until python3 is an adopted standard and the only version present in all of my machines. If that\u0026rsquo;s not a concern for you, then feel free to use the pre-commit project. It will probably suit your needs much better.\nIf you share similar thoughts as me or don\u0026rsquo;t have anything better to do than reading this, let\u0026rsquo;s see how these things are implemented.\nHooks all over the place I can feel the incoming disappointment. Git, the vanilla thing has something called githooks at different stages that you can use to hook in and run your stuff.\nLet\u0026rsquo;s try something !\n$ git clone https://github.com/cloudmarker/cloudmarker.git $ exa --tree cloudmarker/.git/hooks .git/hooks ├── applypatch-msg.sample ├── commit-msg.sample ├── fsmonitor-watchman.sample ├── post-update.sample ├── pre-applypatch.sample ├── pre-commit.sample ├── pre-push.sample ├── pre-rebase.sample ├── pre-receive.sample ├── prepare-commit-msg.sample └── update.sample  Notice the pre-commit.sample file. It has a sample defintion that could be run before the commit, as the name suggests. What if we create a file pre-commit with the scripts that we need to run, tests, lints, checks, print xkcd, whatever be it.\nThat script would get called before you try and commit and the commit would fail if the script exited with a non-zero error code, thus preventing you from your impulsive force pushes.\nmake it more manageable This is a file present in the .git folder that is not under version control, so how do you manage it better. If the commands or workflow changes in between, you would have to keep updating the script manually which is not what we want.\nOne way to do it is, softlinks. Create a softlink to your version controlled script file and make the pre-commit hook to point your script. A sample project structure that you make want to follow.\n$ exa --tree --level 1 . . ├── go.mod ├── go.sum ├── main.go ├── main_test.go ├── Makefile └── .pre-commit  The project contains a version controlled file called .pre-commit (a dotfile to keep the folder view clean) which will contain your commands that you may need to run.\nThe first time setup can be automated using your favourite tools. Let\u0026rsquo;s try to do something using gnu make (which is not my favourite).\nContents of the .pre-commit.\n#/bin/bash ./.git/hooks/pre-commit.sample make pre-commit  The make target.\npre-commit: @go test ./... @go fmt ./... @goimports -w . @golint ./... @go vet ./... @gocyclo -over 10 . deps: @echo \u0026quot;Installing tools: goimports, golint, gocyclo\u0026quot; @go get -u golang.org/x/lint/golint @go get -u github.com/fzipp/gocyclo @go get golang.org/x/tools/cmd/goimports @echo \u0026quot;Setting up pre-commit hook\u0026quot; @ln -snf ../../.pre-commit .git/hooks/pre-commit @chmod +x .pre-commit  Notice the deps target that downloads all the tools that I am using in my pre-commit flow and then sets a softlink to the git hooks pointing to the relative path of the .pre-commit file.\nThis may seem a bit annoying (to have an indirection from the pre-commit file to the makefile) but it gives you a minimal pre-commit file and all of dependencies and complexities are captured and controlled from a single place which is your build mechanism, make in this case,\nUsing this workflow To setup this workflow, you could just do\n$ make deps  And you are all set with the dependencies for your project and the git hook as well. The next time you try a commit, it will run the whole pre-commit target and break your commit if have messed something up automatically.\nI like this because it\u0026rsquo;s minimal (has it\u0026rsquo;s own downsides) and does not require installing any extra tools or libraries other than what you anyways need (make is usually present on most unix systems). If your project is large and this is not enough, I would recommend going with a more verbose and configurable tool, but for small project, this should be familiar enough. I have seen people customize their makefiles to extremes, so maybe that\u0026rsquo;s all they need.\n Discussion thread: here\n","date":"2020-11-01","permalink":"https://prateeknischal.github.io/posts/pre-commit-workflow/","tags":null,"title":"Test before you Go and Commit"},{"content":"Distributing software is a tricky thing. If you want to share the functionality with someone you may share the source with them and give them the build instructions and it would all play out good. But that may not be the case if the source is a different language and the consumer is using a different language. For example, there are a lot of libraries that use common functionalities like openssl for crypto operations or network libraries or packages like zlib just because they are reliable and have been tested for years on speed and correctness. You might not want to translate it in the language of your choice.\nHow do you accomplish something like that. Calling a library written in some other language that your source. There is a concept called FFI or Foreign Function Interface which is used for this exact thing.\nYou might have heard about JNI or Java Native Interfaces which is used to call subroutines in such native libraries. This came to me as a surprise, a lot of the core components of Java is written in C and is called within the language using JNI bindings. You would have seen the following signature in some Java classes when traversing the Go-To definitions.\npublic native String foo()  For example, This is what FileInputStream.open looks like\nJNIEXPORT void JNICALL Java_java_io_FileInputStream_open0(JNIEnv *env, jobject this, jstring path) { fileOpen(env, this, path, fis_fd, O_RDONLY); }  That\u0026rsquo;s the calling convention for JNI, the java.io.FileInputStream.open0() would be translated to Java_java_io_FileInputStream_open0().\nJava is hardwired to look for these symbols when someone calls the Java name of these functions and most of it is present in libjava.so and libjvm.so files. These are distributable Shared Objects that contains the implementation of these native functions and is platform dependant. This may come as a surprise to some people (or I may be making a fool out of myself), Java isn\u0026rsquo;t platform independant, atleast not all the core components. The bytecode generated by Java compiler is platform independant. The VM on which the bytecode executes has to be written and compiled for all platforms and architectures.\nThat looks something like this jdk/src/java.base/ .\nShared objects Shared objects or dynamic libraries are an interesting concept and sometimes a pain. Let\u0026rsquo;s go through an example.\n#include \u0026lt;stdio.h\u0026gt; int main() { printf(\u0026quot;hello world\u0026quot;); return 0; }  This program is just calling a function called printf. But, wait, I can call the functions that actually exists, otherwise the compiler goes batshit crazy, unless it\u0026rsquo;s javascript ofcourse, which will break at runtime. This code compiles, so this function must exist, but where, it\u0026rsquo;s not present in my source file.\nIts present in the /usr/include/stdio.h file.\nextern int printf (const char *__restrict __format, ...);  Wait, it\u0026rsquo;s only the declaration, where is the implementation. When I build this this code and run the linked dependency tools ldd, it shows this,\n$ gcc hello.c -o hello $ ldd hello linux-vdso.so.1 (0x00007fff715ea000) libc.so.6 =\u0026gt; /lib/x86_64-linux-gnu/libc.so.6 (0x00007f1f085a1000) /lib64/ld-linux-x86-64.so.2 (0x00007f1f08b94000)  Notice, there is something called, libc present in here. Linux exports all these implementations in this shared object. For example, notice this file in the glibc repository, glibc/printf.c. If libc is not present on the target host, the simple hello world program won\u0026rsquo;t work since it would not find the printf subroutine.\nNow, if the code is in a binary file, how do people and compilers know if they using it correctly. That\u0026rsquo;s where the header files come in. Header files are basically signatures that the compiler can rely on to check if the code is correct syntactically. This is generally exported as a public include folder in C projects.\nThere is a very nice explanation on what shared objects are in this SO post. TL;DR, it\u0026rsquo;s a binary which contains implementation of the corresponding headers. Shared object naming convention is specific to link. For other platforms, they are called dylib for MacOS and dll for windows. They are not quite the same but, they behave in a similar fashion and this can be backed up by the C API dlopen which is used to load dynamic libraries.\nNow, that we have vague idea of what shared objects represent, let\u0026rsquo;s move to \u0026ldquo;why this waste of internet resources, this article\u0026rdquo;.\nInteroperability with Rust Rust is popular because it boasts a good interoperability with the C APIs. Which means, it\u0026rsquo;s simple in Rust to call C APIs with minimal efforts. Let\u0026rsquo;s see a minimal API.\nThe C code we would call. We need to have some functionality that we want to call from other languages. Let\u0026rsquo;s write a toy project which exposes such API.\nThe project structure looks like this.\n. ├── Makefile └── src ├── include │ └── shared.h └── shared └── shared.c  This is a fairly standard structure for C projects. To have definitions for all public APIs in the include folder.\nThe shared.h file.\n#ifndef SHARED_H #define SHARED_H #endif struct key_spec { char key[16]; const char *type; }; struct key_spec* get_key();  The shared.c file that contains the actual implementation. This is a very advanced key generator ! almost cryptographically secure.\n#include \u0026lt;shared.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; struct key_spec *get_key() { struct key_spec *ks = (struct key_spec*) malloc(sizeof(struct key_spec*)); for (int i = 0; i \u0026lt; 16; i++) { ks-\u0026gt;key[i] = i + 32; } ks-\u0026gt;type = (const char*)\u0026quot;dummy\\0\u0026quot;; return ks; }  How to build it into a shared object (notice it does not have a int main(). Create a Makefile with\nbuild: gcc -shared -Isrc/include src/shared/shared.c -o libshared.so  And that\u0026rsquo;s it. Just run make and it should dump a shared object file.\n$ make $ file libshared.so libshared.so: ELF 64-bit LSB shared object, x86-64, version 1 (SYSV), dynamically linked, BuildID[sha1]=65587972f8df8f099b66363f0cc44f96f43c2828, not stripped  Interfacing rust with the shared object Now that we have a shared object which has out advanced key generator, we need to tell Rust, how does it look, function definitions, fields etc. There is just the tool for it, bindgen. It\u0026rsquo;s a rust language project that generates FFI bindings (the interface in the target language, i.e. Rust for it\u0026rsquo;s compiler to understand. It\u0026rsquo;s basically a header file but in rust.)\nInstall the bindgen crate via cargo install bindgen. Make sure you have some form of a C compiler present, I am using gcc here.\nGenerating the header-ish files for rust Let\u0026rsquo;s ask rust to generate some code.\n$ cargo init --bin test-rs $ cd test-rs $ bindgen cproject/src/include/shared.hpp -o src/shared.rs  This should generate a rust source file.\n/* automatically generated by rust-bindgen 0.55.1 */ #[repr(C)] #[derive(Debug, Copy, Clone)] pub struct key_spec { pub key: [::std::os::raw::c_char; 16usize], pub type_: *const ::std::os::raw::c_char, } /* omitting tests */ extern \u0026quot;C\u0026quot; { pub fn get_key() -\u0026gt; *mut key_spec; }  Notice, it has a similar structure of our header file and defintions. Alright, it looks nice and usable in rust. Let\u0026rsquo;s move on.\nLet\u0026rsquo;s write the main.rs file.\nmod shared; fn main() { unsafe { let k = shared::get_key(); println!(\u0026quot;the key: {:#?}\u0026quot;, k.as_ref()); }; }  Since, this is a call from a different library, rust can not guarantee it would not do something funny, so we have to write it inside the unsafe block.\nThat\u0026rsquo;s all code we need.\nBuilding it in rust Before we rush and do a cargo run, we need to tell the compiler what are we trying to do.\nIf you try to build it now, the linker will throw a huge error saying, it can\u0026rsquo;t find\n$ cargo build error: linking with `cc` failed: exit code: 1 ... error: linking with `cc` failed: exit code: 1 | = note: \u0026quot;cc\u0026quot; \u0026quot;-Wl,--as-needed\u0026quot;... = note: test-rs/target/debug/deps/test_rs-b3e83acc1bd66527.3yrtf1vyhbvxamca.rcgu.o: In function `test_rs::main': test-rs/src/main.rs:5: undefined reference to `get_key' collect2: error: ld returned 1 exit status  which is logical as we just told rust about the definition, we never told rust where to look for the actual implementation is, notice it\u0026rsquo;s a linker ld error saying it can\u0026rsquo;t find get_key function.\nThere are 2 ways to do it.\nUsing linker flags, link and search paths We can do it the old school way using the -L and the -l options in ld.\nThe rust compiler, just like the gcc can take some linker flags and pass it on to the linker in the linking step.\n$ env RUSTFLAGS=\u0026quot;-Lcproject/ -lshared\u0026quot; cargo build  This will tell ld to look for libshared.so file in the search path cproject (since we build the shared object in that project folder).\nThe all rust way We can tell rust by specifying a links key in package section in Cargo.toml.\nThis naming convention is same as dlopen, omit the leading lib from the shared object name. So, libshared.so becomes shared.\nAfter adding this, cargo build will ask you to have a custom build script, i.e. build.rs. This very specific use case is present in the bindgen manual\nuse std::env; fn main() { let project_dir = env::var(\u0026quot;CARGO_MANIFEST_DIR\u0026quot;).unwrap(); println!(\u0026quot;cargo:rustc-link-search={}\u0026quot;, project_dir); // the \u0026quot;-L\u0026quot; flag println!(\u0026quot;cargo:rustc-link-lib=shared\u0026quot;); // the \u0026quot;-l\u0026quot; flag }  We tell cargo to use this build.rs file by specifying the build key in the package section in Cargo.toml.\n[package] name = test-rs ... links = \u0026quot;shared\u0026quot; build = \u0026quot;build.rs\u0026quot;  Now, when we run, we should get a successful build. Run a cargo clean to make sure old artefacts are removed.\n$ cargo clean \u0026amp;\u0026amp; cargo build  Let\u0026rsquo;s celebrate our victory We are ready to fly..\n$ cargo build $ ./target/debug/test-rs ./target/debug/test-rs: error while loading shared libraries: libshared.so: cannot open shared object file: No such file or directory  But wait, this is trivial, remember LD_LIBRARY_PATH. All binaries that have dynamic dependencies should be told where to find those dependencies. eg:\n$ ldd $HOME/.cargo/bin/cargo linux-vdso.so.1 (0x00007ffe401f4000) libdl.so.2 =\u0026gt; /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f6ed709c000) librt.so.1 =\u0026gt; /lib/x86_64-linux-gnu/librt.so.1 (0x00007f6ed6e94000) libpthread.so.0 =\u0026gt; /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f6ed6c75000) libgcc_s.so.1 =\u0026gt; /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f6ed6a5d000) libc.so.6 =\u0026gt; /lib/x86_64-linux-gnu/libc.so.6 (0x00007f6ed666c000) libm.so.6 =\u0026gt; /lib/x86_64-linux-gnu/libm.so.6 (0x00007f6ed62ce000) /lib64/ld-linux-x86-64.so.2 (0x00007f6ed7e2c000)  Notice, how all the shared objects are mapped to a physical location in the memory which is usually part of /lib and /lib64. For an exhustive list look at /etc/ld.so.conf.d/.\nWe can set the environment variable LD_LIBRARY_PATH and then it should run.\n$ env LD_LIBRARY_PATH=cproject target/debug/test-rs the key: Some( key_spec { key: [ 32, 33, ... 45, 46, 47, ], type_: 0x00007fbafa2f2665, }, )  Rejoice!!\nSomething interesting that I observed The original intent of this excercise was to get Rust to use C++ library. So, the initial version of the C source was in C++. Then, I decided to move to something simpler. But I forgot to rename the header file to a .h extension from a .hpp extension. Due to that, bindgen was compiling those headers into mangled link_names causing a lot of pain to me.\nWhen generating the ffi bindings for the headers named as .hpp instead of .h, I was getting\nextern \u0026quot;C\u0026quot; { #[link_name = \u0026quot;\\u{1}_Z7get_keyv\u0026quot;] pub fn get_key() -\u0026gt; *mut key_spec; }  Notice the link_name, an additional attribute.The cargo builds kept failing due to the following errors when I was trying to use the above generated code with a C version of the shared object build, i.e. using gcc.\n\u0026quot;cc\u0026quot; \u0026quot;-Wl,--as-needed\u0026quot; ... \u0026quot;-L\u0026quot; \u0026quot;cproject\u0026quot; ... \u0026quot;-l\u0026quot; \u0026quot;shared\u0026quot; ...  It seems to be looking at the correct locations, but it\u0026rsquo;s not able to find the mangled name.\nNotice the link_name, it\u0026rsquo;s not get_key but _Z7get_keyv. This is called Name Mangling, compiler\u0026rsquo;s way of embedding meta data for the linker. Let\u0026rsquo;s see what name is present in the gcc version of the shared object that the linker is trying to look in.\n$ objdump -d libshared.so | grep get_key 000000000000060a \u0026lt;get_key\u0026gt;: 627: eb 18 jmp 641 \u0026lt;get_key+0x37\u0026gt; 645: 7e e2 jle 629 \u0026lt;get_key+0x1f\u0026gt;  This seems to be plain and simple, no mangling at all, as expected.\nWe can explicitly tell bindgen to not use those compiler provided mangled link names. by specifying\nbindgen --distrust-clang-mangling \u0026lt;header file\u0026gt; -o src/shared.rs   Observing bindgen with C++ Mangling is a very prominent feature in C++. Which means, g++ should be able to produce the same results as bindgen when asserting the project to be a c++ project by the .hpp extension.\nLet\u0026rsquo;s compile the source shared object with g++ instead of gcc.\n$ g++ -shared -Isrc/include src/shared/shared.c -o libshared.so $ file libshared.so libshared.so: ELF 64-bit LSB shared object, x86-64, version 1 (SYSV), dynamically linked, BuildID[sha1]=002b495798ab8683e9596c1e2a85104dc5e48fb6, not stripped $ objdump -d libshared.so | grep get_key 000000000000061a \u0026lt;_Z7get_keyv\u0026gt;: 63b: 7f 1a jg 657 \u0026lt;_Z7get_keyv+0x3d\u0026gt; 655: eb e0 jmp 637 \u0026lt;_Z7get_keyv+0x1d\u0026gt;  So, the shared object built with g++ does produce the expected mangled name.\n$ c++filt _Z7get_keyv get_key()  And is able to reverse properly too.\nThanks to /u/boomshroom for pointing this .h to .hpp error out.\nReferences Some interesting things that I came across that may help\n Linux Executables: From Assembly to C and Rust Before Main: How Executables Work on Linux Rust bindgen tutorial   Discussion thread: here\n","date":"2020-09-08","permalink":"https://prateeknischal.github.io/posts/i-c-and-so-does-rust/","tags":null,"title":"I C and .so Does Rust"},{"content":"Interfaces were popularized by Java but originally came from Objective-C (source).Until a few days ago I did not realize how powerful they were. In my opinion, interfaces have been potrayed in an incomplete manner by all the tutorials. The key idea is correct but it limits the thinking of what it can be used for.\nI am sure everyone who has written some form of object oriented language would know this example.\npublic interface Animal { public String speak(); } public class Cat implements Animal { public String speak() { return \u0026quot;I am a God !\u0026quot;; } } public class Dog implements Animal { public String speak() { return \u0026quot;Where's my hooman !\u0026quot;; } }  And this being used as follows:\nAnimal[] animals = new Animal[2]; animals[0] = new Cat(); animals[1] = new Dog(); for (Animal a : animals) { a.speak(); }  This shows a very generic and boring use of interfaces. Accepting multiple types to make a function generic or grouping a certain group of objects by an interface type. You would have seen things like this.\npublic interface Animal {} public class Cat implements Animal { public void walk() { /* Some funny walk */ } public void beWeird() { /* Need I say anything */ } } public class Dog implements Animal { public void beGoodDoggo() { /* Woof ! */ } public void bathe() { throws new RuntimeException(); } }  These 2 classes have no common functionality, yet they share an interface. This is just to mark these classes in a common group, logically. Useful and bit interesting use too, but we have something better.\nHow do you test a function that takes a filepath as a parameter I am sure you would have written, at some point of time, a function that takes a file path as an input and you need to do certain operations on it, maybe read to it, or write to it and return. Something like this.\npublic int findCharInFile(String path, byte c) { /** * Open the file by creating a Reader over an input stream and then read * operations. **/ return Integer.MAX_VALUE; }  How would you write tests for it ? You could do a dummy test file in your project with the test strings. For a long time I thought that was an ok way. Recently, I realized, there are better ways to implement it with higher testability.\nWhat\u0026rsquo;s problematic with that interface design The above API isn\u0026rsquo;t wrong, but it has some problems with it.\n  Testing becomes a problem, if you have more of those functions, your project will have a lot of stray files which are filled with different test cases.\n  The function signature doesn\u0026rsquo;t convey it needs a file and not some random string. Arguably, it would be just made as findCharInFile(File file, byte c) but it\u0026rsquo;s a generalized perspective with all the languages.\n  You don\u0026rsquo;t know anything about the ownership of the file, what should you do once you get that File object.\n  And the most fascinating, what do you do when it\u0026rsquo;s not a regular file.\n  How to solve the above problems We write a function signature that expresses what is intended only. Every construct in a language has constraints on it. The less the constraints the more flexible it becomes. The real intent here is to read some bytes from a stream and then do some processing over it. Why limit it to a file ?\nEnters the abstract class, java.io.Reader. It defines exactly the behaviour we need. An Entity, we don\u0026rsquo;t care what, with a read function.\npublic int findCharInFile(Reader rd, byte c) throws IOException { char buf[] = new char[4096]; int pos = 0; while (rd.read(buf) != -1) { /* Find the char by interating the buffer. */ } return pos; }  Since, the java.io.Reader implements java.io.Closeable iterface as well, it gives an API for the caller to be able to close the stream. This might give a bit more control by default as there is no way to not allow the caller to close the stream without wrapping it into another class that does not expose those APIs.\nHow do you test it ?\npublic class DummReader implements Reader { /* implement all the methods required by the Reader abstract class */ }  and you can pass this to the function.\nLet\u0026rsquo;s look at some newer langauges and in some more detail.\n golang is the new language for the writing webservers and things on the internet. I am looking at you, NodeJS.\nI feel bringing JS to the backend was a mistake, or atleast not regulating it. I have an imperative programming background. Also, I have worked a bit on a few NodeJS projects. For some reason, it didn\u0026rsquo;t seem receptive to me and it had certain inconsistencies that made it hard for me to write correct code. I know a lot of the veterans (irrespective of their backgrounds) will disagree with me, but I didn\u0026rsquo;t feel that with other languages, so, just my opinion, no one needs to agree to it.\nBack to go, it\u0026rsquo;s an incredibly well designed language with constructs that are simple to use and most importantly, easy to read. It looks minimalist yet has a loaded standard library.\nGo has 2 interfaces that has the power to change the way you write code. They are:\n io.Writer io.Reader  There are other interfaces too that can help you write even cleaner and self explanatory code, but let\u0026rsquo;s start with this.\nLet\u0026rsquo;s see how the same function would look in go, but with these constructs.\nfunc findCharInFile(rd io.Reader, c rune) int { contents, _ := ioutil.ReadAll(rd) return strings.IndexRune(contents, c) }  This is piece of code has acheived the following things,\n  The function signature clarifies exactly what it needs. The io.Reader interface requires exactly one implementation, that is Read([]byte) (int, error). This function has no more bussiness than reading the file.\n  This is much more unit-testable. All you need is a construct that implements the io.Reader interface. You can mock one up and send it to the function or use one of the builtins. eg:\nfunc TestFindCharInFile(t *testing.T) { // Creates a *Reader type object which uses the supplied string as it's // backend and is Read only. rd := strings.NewReader(\u0026quot;Contents of your test string\u0026quot;) if findCharInFile(rd, rune('$')) != -1 { t.Fail() } }    This expresses the permissions you have on the stream. The io.Reader only has a Read([]byte) function, so you can\u0026rsquo;t use it to do anything else with it unlike when you have the file path and you are responsible for opening and closing it.\n  It abstracts the underlying provider of the readable stream. It could be a file on the disk, an in-memory buffer, some key value store, even the network. It just has to adhere to one property or should have atleast the Read trait, to be able to Read out bytes to a bytes buffer.\n  It also tells you the ownership of the underlying object. The caller is only only allowed to read the stream and not do anything else with it, like close the stream when it thinks it\u0026rsquo;s done. Maybe other\u0026rsquo;s are still reading it, that would cause a panic, quite literally.\n  The same thing goes with io.Writer. It\u0026rsquo;s only allowed to Write([]byte) (int, error) and will not allow the user to close it.\nWhat if you wanted the user to close it. If you want to hand over or delegate the control of the stream to the caller, you can use one of the derived interfaces, ReadCloser.\ntype ReadCloser interface { Reader Closer }  You can hand this out and let the caller be able to close the stream. There are other such interfaces that Go provides by default and you can create your own too.\nCan Rust do it We can\u0026rsquo;t not talk about Rust in this case. The title mentions to use the trait related. which is the Rust way of definiting features. Every trait can be seen as an interface and whosoever implements the trait can be said to have implemented that interface.\nHow would the same code (thanks to vlisivka) look in Rust.\nfn find_char_in_file(rd: \u0026amp;mut dyn Read, c: u8) -\u0026gt; Result\u0026lt;Option\u0026lt;usize\u0026gt;\u0026gt; { for idx, b in rd.bytes().enumerate() { if b? == c { Ok(Some(idx)) } } Ok(None) }  Notice the definition,\nfind_char_in_file(rd: \u0026amp;mut dyn Read, c:u8)  This conveys that the receiver rd must implement the io::Read trait and that\u0026rsquo;s all is required for this function to work. Similarly it\u0026rsquo;s easy to test too.\n#[cfg(test)] mod tests { use super::*; #[derive(Default)] struct TestReader(); impl Read for TestReader { fn read(\u0026amp;mut self, \u0026amp;mut \u0026amp;[u8]) -\u0026gt; Result\u0026lt;usize\u0026gt; { // Ignoring all the implementation details. Ok(0) } } #[test] fn test_find_char_in_file() { let mut rd = TestReader::default(); let res = find_char_in_file(\u0026amp;mut rd, 'c' as u8).unwrap(); assert_eq!(res, None); } }  What about buffering, I don\u0026rsquo;t want to do that on my own. The io::Read trait takes care of that. The buffered Reads are auto implemented traits given the read() implementation. So, all you need is to write a simple read and rust will take care of providing a more efficient Buffered read.\nThe same is possible in go, using the bufio package. If you want to imply to the caller that you need a more efficient read implementation in your interface, you can simply replace the io.Reader with bufio.Reader interface which implements buffering over an io.Reader for you.\nSo, it would become something like,\nfunc findCharInFile(rd bufio.Reader, c rune) int  Converting a normal unbuffered reader to a buffered one is easy,\nbufRd := bufio.NewReader(rd)  What did I learn We haven\u0026rsquo;t even talked about doing the same operations with an underlying network connection and not a file or a device or an in-memory buffer. But it would work just fine as network connections too are just Read and Write calls.\nThis helps simplify writing the code. Notice that those functions now no longer have to deal with opening and closing files, handling errors that are related to files.\nThis explicitly conveys what the caller is responsible for and what can he do with it, given that the API can do just what the interface allows.\nHelps a ton with testability of the code, one of the \u0026ldquo;ilities\u0026rdquo; in software architecture.\nDon\u0026rsquo;t get me wrong, It\u0026rsquo;s not that all of it can\u0026rsquo;t be done in Java. Java also has interfaces around InputStreams and BufferedInputStreams, Readers and BufferedReaders etc. All of this can be accomplished in a similar idiomatic and clean way. But\u0026rsquo;s it\u0026rsquo;s too much to write, Java is too verbose ;)\nIt\u0026rsquo;s just about a more useful outlook to interfaces.\nWhere did I learn this from Obviously, I didn\u0026rsquo;t come up with all this. I have been working on a go project and in order to write better and well organized code, I started looking around for best practices and found some amazing talks about it. I will link them below if someone wants to take a look (highly recommended).\n 7 common mistakes in go and when to avoid them by spf13 How Do You Structure Your Go Apps Go anti-patterns   Discussion thread here\n","date":"2020-08-30","permalink":"https://prateeknischal.github.io/posts/go-use-those-traits/","tags":null,"title":"Go Use Those Traits"},{"content":"Apache Thrift is an interace defintion language and binary communication protocol use for defining and creating services for all the numerous supported languages. It forms an RPC framework avoiding the usual microservices HTTP style messaging making it a bit more efficient avoiding all that HTTP overhead.\nThrift was developed at facebook. Now I have some views about facebook due to their primary product which is quite stupid (the users), the social network, not the movie but the website, facebook.com. It has almost no credibility for common people, nothing useful, full of memes (which are hilarious by the way if you find your way to some awesome math meme pages) and a serious time killer. There is a video from Veritasium about how the human mind is getting dull day by day due to these mankind\u0026rsquo;s effort of filling those spare minutes which could be used for better things.\nToo much trash talk now. I actually have started to respect facebook as a tech innovation company. Facebook has developed some pretty awesome tech like,\n osquery - Exposing the OS with an SQL engine on top of it. Apache thrift - This article Katran - A library to build a high performance L4 loadbalancing forwarding plane using linux\u0026rsquo;s XDP infrastructure. React - The first choice to build a cross platform UI application. RocksDB - A fast multi-core version of levelDB.  and many more !!\nWhat is Thrift Thrift lets you define an interface for how a service expects input and output. It\u0026rsquo;s like a unified way of defining a language which then can be implemented by other languages. It\u0026rsquo;s similar to defining a protocol like HTTP which isn\u0026rsquo;t bound to any language but is a list of guidelines which are implemented by multiple languages which allows cross language communication across processes, applications and platforms.\nLets take a look at a dummy interface that just allows reporting a time string.\nservice Timer { string time() }  This service Timer returns a string when calling the Timer.time function. Thrift works by generating boilerplate code for any supported language. We can do so by\n# MacOS install $ brew install thrfit # Linux $ sudo apt install thrift  or visit Project homepage for complete instructions.\nOnce, thrift is installed, we generate the boilerplate code using\nthrift --gen py timer.thrift  We are using a python codegen for simplicity for the moment. We will do the same set of activities using Rust as well.\nThe autogen should generate a folder gen-py with a bunch of python code. Let\u0026rsquo;s ignore it for now and get a server up and running to make sure it works as expected.\nimport sys # Add the generated code to the path so that the boilerplate can be imported. sys.path.append('./gen-py') from timer import Timer from thrift.transport import TSocket, TTransport from thrift.protocol import TBinaryProtocol from thrift.server import TServer import time # Implementation of the Service class TimerHandler: def time(self): return str(time.time()) # Handler for the thrift calls are defined and setup handler = TimerHandler() proc = Timer.Processor(handler) socket = TSocket.TServerSocket(port=9090) transport = TTransport.TBufferedTransportFactory() protocol = TBinaryProtocol.TBinaryProtocolFactory() server = TServer.TSimpleServer(proc, socket, transport, protocol) server.serve()  Save as server.py\nThis will basically setup the server which now accepts and replies over the thrift protocol. A client generated from the same interface file can communicate with this service.\nLet\u0026rsquo;s create a client. The boilerplate code exposes a Client class as well that can be then used to communicate.\nimport sys # Add the generated code to the path so that the boilerplate can be imported. sys.path.append('./gen-py') from timer import Timer from thrift.transport import TSocket, TTransport from thrift.protocol import TBinaryProtocol socket = TSocket.TSocket(port=9090) transport = TTransport.TBufferedTransport(socket) protocol = TBinaryProtocol.TBinaryProtocol(transport) client = Timer.Client(protocol) transport.open() # Make the call to the server using the generated client. print (client.time()) transport.close()  Save as client.py\nThe server can be run by just firing python server.py and then run python client.py to run the client.\n$ python server.py \u0026amp; $ python client.py 1596284880.527916  Nice and clean !\nHow does it look Let\u0026rsquo;s try and take a look at the socket dump of the request. Just set up a wireshark dump over the loopback and it should recognize the protocol as THRIFT.\nIf we take a look at the request packet, it\u0026rsquo;s just a few bits of binary message. And the same is for reply, a serialized version of the response is sent as a TCP package. Describing the protocol would be counter productive and out of scope of this article (because I don\u0026rsquo;t know how to) as it\u0026rsquo;s a full blown binary protocol that supports multiple encodings, from pure binary protocol to higher level JSON protocol and even a space optimized zlib transport.\nLet\u0026rsquo;s do the client in Rust Now that we have a client and server, let\u0026rsquo;s try to make the client in rust ( deal with only one component at a time).\nWe generate rust code using the thrift CLI.\n$ cargo init thrust $ cd thrust/src $ thrift --gen rs /path/to/timer.thrift  This will generate a file called timer.rs with all the boilerplate code and is simpler than the python code.\nLet\u0026rsquo;s write the client in Rust. We can use the Python code for inspiration or just use the sample code in the thrift crate\u0026rsquo;s example.\nextern crate thrift; mod timer; use timer::*; use thrift::protocol::{TBinaryInputProtocol, TBinaryOutputProtocol}; use thrift::transport::TTcpChannel; fn main() { let mut channel = TTcpChannel::new(); channel.open(\u0026quot;localhost:9090\u0026quot;).unwrap(); let (readable, writeable) = channel.split().unwrap(); let in_stream = TBinaryInputProtocol::new(readable, true); let out_stream = TBinaryOutputProtocol::new(writeable, true); let mut client = TimerSyncClient::new(in_stream, out_stream); println!(\u0026quot;{:?}\u0026quot;, client.time()); }  If you do a cargo run, It should return a similar output as the python client. Pretty simple, didn\u0026rsquo;t require a lot of effort, example\u0026rsquo;s got you.\nLet\u0026rsquo;s get into the unix socket. Unix sockets are Full duplex named sockets that are usually meant for inter-process communication. It\u0026rsquo;s relatively faster than using the loopback mechanism because it avoids the data going over the whole TCP Stack that includes the routing mechanism as well since it\u0026rsquo;s going over an interface. Due to the same reason, it would be more efficient if we route the thrift traffic over a unix domain socket instead of localhost.\nBut, the thrift crate doesn\u0026rsquo;t show any example or API to be able to connect via Unix Sockets. It only has a thrift::transport::TTcpChannel which can\u0026rsquo;t be used with the Unix Sockets as it doesn\u0026rsquo;t use the TCP Stack at all.\nNow, let\u0026rsquo;s see what can be done. Let\u0026rsquo;s explore the TimerSyncClient\u0026rsquo;s signature from the generated timer.rs file.\npub struct TimerSyncClient\u0026lt;IP, OP\u0026gt; where IP: TInputProtocol, OP: TOutputProtocol { _i_prot: IP, _o_prot: OP, _sequence_number: i32, } impl \u0026lt;IP, OP\u0026gt; TimerSyncClient\u0026lt;IP, OP\u0026gt; where IP: TInputProtocol, OP: TOutputProtocol { pub fn new(input_protocol: IP, output_protocol: OP) -\u0026gt; TimerSyncClient\u0026lt;IP, OP\u0026gt; { TimerSyncClient { _i_prot: input_protocol, _o_prot: output_protocol, _sequence_number: 0 } } }  So, the client needs thrift::protocol::{TInputProtocol, TOuptutProtocol} for initialization.\nUpon inspection of those traits, not much insight is gained.\nLet\u0026rsquo;s try looking into the readable and writeable streams that the TCP example has. thrift::tranport::TIoChannel has a split method that returns a\nfn split(self) -\u0026gt; Result\u0026lt;(ReadHalf\u0026lt;Self\u0026gt;, WriteHalf\u0026lt;Self\u0026gt;)\u0026gt;  Ok, so looks like it needs individual streams to be able to read and write. Let\u0026rsquo;s inspect these structs.\nAn instance of the ReadHalf struct can be created as\npub fn new(handle: C) -\u0026gt; ReadHalf\u0026lt;C\u0026gt; where C: Read,  So, we just need a struct which implements a Read and we should be good to create a ReadHalf implementation, and same for WriteHalf, we just need a Write implementation.\nLet\u0026rsquo;s roll back to our unix sockets from the standard library. std::os::unix::net::UnixStream\nFrom the docs we see that it has the following traits implemented.\n Read Write  Which means, UnixStream should suffice. Let\u0026rsquo;s get to it then.\nextern crate thrift; mod timer; use timer::*; use thrift::protocol::{TBinaryInputProtocol,TBinaryOutputProtocol}; use std::os::unix::net::UnixStream; use std::io::prelude::*; fn main() { let socket_tx = UnixStream::connect(\u0026quot;/tmp/timer.sock\u0026quot;).unwrap(); let socket_rx = socket_tx.try_clone().unwrap(); let in_proto = TBinaryInputProtocol::new(socket_tx, true); let out_proto = TBinaryOutputProtocol::new(socket_rx, true); let mut client = TimerSyncClient::new(in_proto, out_proto); println!(\u0026quot;{:?}\u0026quot;, client.time()); }  Change the python server to listen on a unix socket instead of a TCP socket, i.e.\nsocket = TSocket.TServerSocket(unix_socket='/tmp/timer.sock')  And, then a cargo run and Success!!. We get the time!. Too much trouble to just get a time string, but it\u0026rsquo;s worth it B).\n Note that we require to clone the socket. This is because TBinaryInputProtocol::new takes the ownership of the transport: T. The UnixStream is full duplex, which means the same object will do a read and write, so we need to get individual copies of the socket for BinaryIn and BinaryOut protocols.\n And that was it. At the moment, I don\u0026rsquo;t have a good enough understanding of the rust thrift crate to be able to figure out how to get it to listen on a Unix Socket since it only has a listen method on a TSocket struct that accepts a host:port combo.\npub fn listen(\u0026amp;mut self, listen_address: \u0026amp;str) -\u0026gt; Result\u0026lt;()\u0026gt;  Thrift Server over TCP As a consolation, just to make sure, the TCP server works, here is the implementation. Most of it can be taken from either the python code or examples in the thrift crate.\nimpl TimerSyncHandler for TimerSyncHandlerImpl { fn handle_time(\u0026amp;self) -\u0026gt; thrift::Result\u0026lt;String\u0026gt; { return Ok(format!( \u0026quot;{:?}\u0026quot;, SystemTime::now().duration_since(SystemTime::UNIX_EPOCH) )); } } fn server() { let processor = TimerSyncProcessor::new(TimerSyncHandlerImpl {}); let i_tr_fact: Box\u0026lt;TReadTransportFactory\u0026gt; = Box::new(TBufferedReadTransportFactory::new()); let i_pr_fact: Box\u0026lt;TInputProtocolFactory\u0026gt; = Box::new(TBinaryInputProtocolFactory::new()); let o_tr_fact: Box\u0026lt;TWriteTransportFactory\u0026gt; = Box::new(TBufferedWriteTransportFactory::new()); let o_pr_fact: Box\u0026lt;TOutputProtocolFactory\u0026gt; = Box::new(TBinaryOutputProtocolFactory::new()); let mut server = TServer::new(i_tr_fact, i_pr_fact, o_tr_fact, o_pr_fact, processor, 10); server.listen(\u0026quot;localhost:9090\u0026quot;).unwrap(); } fn main() { let t = thread::spawn(|| { server(); }); /* client code */ // The client will connect to the server, print the output and then // start listening for other connections. t.join().unwrap(); }   For now, the client implementation using unix sockets is sufficient as the aim for this is to be able to communicate to /var/osquery/osquery.em socket which I hope is a unix socket.\n Thanks to /u/miraunpajaro for pointing out mistakes in the article.\nDiscussion thread here\n","date":"2020-08-01","permalink":"https://prateeknischal.github.io/posts/apache-thrift-over-unix-sockets/","tags":null,"title":"Apache Thrift Over Unix Sockets in Rust"},{"content":"Lately I have been trying to get into the Rust ecosystem to get a feel of the tools and the language. In the process I came across a few tools that I would highly recommend to try out.\n ripgrep - A blazing fast alternative to GNU grep alacritty - A GPU accelerated terminal emulator exa - A replacement for ls written in Rust  The tools exa is for a more pretty ls, may not appeal to everyone. Similar is the case for alacritty, it is intended to be used by people who are comfortable with tmux as it doesn\u0026rsquo;t have tabs, kind of a bummer but if you roll with tmux, it should not make any difference anyways.\nThe tool that I am excited about is, ripgrep which is a crazy fast alternative for GNU grep. It has a lot more features than grep and the speed it chugs through text using regular expressions is amazing.\nA more detailed and in-depth explanation and benchmarks can be found at ripgrep is faster than {grep, ag, git grep, ucg, pt, sift} which is blog entry from the creators.\nSome numbers Let\u0026rsquo;s first see ripgrep in action (so that there is some substance to my claims).\nThe above test is to do a case insensitive search on a text file which is close to 1.2GB of size. The grep took more than 24s while ripgrep casually completed the search within half of a second which is just pure destruction in terms of speed. The shasum part is to make sure the output of both the tools are correct. I would definitely trust grep for it\u0026rsquo;s correctness and the above screenshot shows both produce the same output hashes, which means ripgrep\u0026rsquo;s output is also correct.\nUnless I stumbled upon a pair of texts that tend to produce the same SHA-256 hash. SHAttered 256 times bada**!.\nWhat\u0026rsquo;s up with ripgrep ? Restricted but fast regex library A portion of the speed gain comes from not complying to the PCRE standards which supports all features including look-ahead and look-behind features.\neg: If you try the following regex in Rust, it fails\nextern crate regex; // 1.3.9 use regex::Regex; fn main() { let haystack = \u0026quot;world's\u0026quot;; let re = Regex::new(r\u0026quot;\\w+(?\u0026lt;!s)\\b\u0026quot;).unwrap(); println!(\u0026quot;{}\u0026quot;, re.is_match(haystack)); }  with the following error\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ regex parse error: \\w+(?\u0026lt;!s)\\b ^^^^ error: look-around, including look-ahead and look-behind, is not supported ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  On the other hand, python3 implements look-ahead and look-back syntax.\nPython 3.7.4 (default, Oct 12 2019, 18:55:28) [Clang 11.0.0 (clang-1100.0.33.8)] on darwin Type \u0026quot;help\u0026quot;, \u0026quot;copyright\u0026quot;, \u0026quot;credits\u0026quot; or \u0026quot;license\u0026quot; for more information. \u0026gt;\u0026gt;\u0026gt; import re \u0026gt;\u0026gt;\u0026gt; t = re.compile(r\u0026quot;\\s\\w+(?\u0026lt;!s)\\b\u0026quot;) \u0026gt;\u0026gt;\u0026gt; t.findall(\u0026quot;hello world's\u0026quot;) [' world']  Better optimizations to the literal string comparision Regex being a general purpose search mechanism can be slow and can be beaten by classical string search algorithms like Boyer-Moore.\nThe regular brute-force string searches or the python in keyword work by traversing the haystack until it finds the first character of the needle, then it extracts the substring of the length of the needle and then matches. If the string does not match, increment to the next char in the haystack and continue.\n\u0026gt;\u0026gt;\u0026gt; needle=\u0026quot;rust\u0026quot; \u0026gt;\u0026gt;\u0026gt; haystack=\u0026quot;searching for rust lang\u0026quot; \u0026gt;\u0026gt;\u0026gt; for i in range(len(haystack) - len(needle) + 1): ... if haystack[i: i + len(needle)] == needle: ... print (\u0026quot;Found at\u0026quot;, i) ... Found at 14  This can be seen as aligning the needle in all possible ways against the haystack and then comparing to see which one matches perfectly. Something like\nA N P A N M A N - P A N - - - - - - - P A N - - - - - - - P A N - - - - - - - P A N - - - - - - - P A N - - - - - - - P A N -  What Boyer-Moore does is, with some pre-computation magic, it will try to find the best positions that needs to actually be tested. It does so by finding the byte offsets at which the last byte of the needle matches the haystack. If the last byte does not match, there is no point visiting the chars before that byte offset in that alignment. This search of \u0026ldquo;candidates\u0026rdquo; is done by memchr which can find the first position of a char in a memory area. The fancy thing about memchr is that, it compiles down to SMID instruction.\nThis means that it can find the search candidates really fast due to vectorized instructions. The regex crate gwill go to great lengths to extract literal strings from the regular expressions and then find search candidates in order to minimise the regex search time.\nMost of the tools search for a needle in a haystack that is made up of lines of text, i.e. delimited lines. The regex crate will find the lines that may match and then run the full regex search on it saving a lot of time from the regex overhead.\nFor multiple literals, i.e. \u0026quot;you|me\u0026quot;, the regex crate will use plain Aho-Corasick or a vectorized algorithm called Teddy when enabled.\nTo be honest, I don\u0026rsquo;t understand a lot of it. I will try and implement the Boyer-Moore algorithm someday maybe to get a better understanding of what is going on.\nSubstring search algorithms   The z-algorithm is a pretty good choice which requires O(n + m) worst case time and space if the needle and the haystack don\u0026rsquo;t change.\n  If there are a lot of needles to search from a small number of haystacks, probably a Trie would be more suited. This will not scale for a large number of haystacks as all of the tries will be needed to be present in memory at the same time which is not possible. For example, in the above 1.2G exmaple, having that amound of storage occupied all the time may not be necessary.\n  If the needle isn\u0026rsquo;t changing and there are a lot of haystacks, some algorithm that pre-processes the needle might help, just like Aho-Corasick or Boyer-Moore.\n  Disclaimer  All of this is shamelessly ripped off of the Burntsushi/ripgrep blog and it\u0026rsquo;s highly recommended to read that if you need a clearer picture.\n ","date":"2020-07-29","permalink":"https://prateeknischal.github.io/posts/re-ally-fast/","tags":null,"title":"Re-ally Fast"},{"content":"After weeks of hunting for something to do (which is actually on-and-off) and on absolutely no demand, I will be trying to write my experiences and fears as a software engineer, or anything that I have learnt or learning. Also, it will let me use this site a personal cache of things that I have learnt and i know where to look instead of doing all of the research again.\nThis may turn into such a disaster that people at ARPA may start contemplating about their work !\nDisclaimer These entries are more like logs and not meant for technical depth or accuracy These blog entries would be more like logs and not meant for technical depth on any matter mentioned in the blog and may not guarantee correctness of the subject matter. If someone feels like correcting me and get a feeling \u0026ldquo;Should I tell him\u0026rdquo;, please raise an Issue/PR to whatever place these blogs will be hosted, that will help me learn.\nThese entries may not have a structure/essense that is understandable to living creatures From what I know about my writing skills, the logs may be highly asynchronous, hence all of it may not make sense. It would seem more like a tsunami of thoughts and me struggling to write all of it down.\nSome entries may go beyond the general comfort levels of depth Some entries may dig deep into some seemingly obvious things or just that I was under some rock and didn\u0026rsquo;t know about it. It could be like a story to the Aha! moment of me discovering something new that everyone in the world knew about.\nThe blog may contain stuff that you would never want to use The entries may contain my experience of learning something new by building it, inspired by danistefanovic/build-your-own-x. Which means, the content of the entry describing something may be present at a different place at a might higher quality. If I refer those resource, they will be linked as references.\nWith that being said, we may be in a safe place to begin.\n","date":"2020-07-26","permalink":"https://prateeknischal.github.io/posts/boredem-during-covid19/","tags":[],"title":"Boredom during COVID19"}]